# ===================================================================
# STANDARDIZED TEMPLATE CSPBench - UNIFIED STRUCTURE v0.8
# ===================================================================
# This template defines the standard structure for all batch types.
# Infrastructure settings (paths, directories) are configured in .env
# This template only contains execution-specific configurations
# 
# Supported types: execution, optimization, sensitivity
# Usage: Copy this template and adapt for your specific needs
#
# =====================================================================
# SECTION 1: METADATA (REQUIRED FOR ALL)
# =====================================================================
# This section contains basic information about the batch.
# All fields are required for traceability and documentation.
metadata:
  name: "Sensitivity Example"                  # Descriptive batch name (string)
  description: "Detailed description of what the batch does"
    # Complete description (string)    
  author: "Author Name"               # Responsible author (string)
  version: "1.0"                      # Batch version (string)
  creation_date: "2025-07-13"         # Creation date in YYYY-MM-DD format
  tags: ["tag1", "tag2"]              # Tags for categorization (list)

# =====================================================================
# SECTION 2: DATASETS (STANDARDIZED FOR ALL)
# =====================================================================
# Defines the datasets available for use in the batch.
# Each dataset has a unique ID that can be referenced in executions.
# Supported types: synthetic, file, entrez
datasets:
  # === SYNTHETIC DATASETS ===
  # Generate artificial data for testing and development
  - id: "synthetic_small_dataset"                  # string: Unique dataset ID
    name: "Small"              # string: Descriptive name
    type: "synthetic"                 # string: Dataset type
    parameters:                       # Parameters specific to synthetic datasets:
      # Modo de geração: "random" | "noise" | "mutations" | "clustered"
      mode: "random"

      # Parâmetros comuns
      n: 8
      L: 10
      alphabet: "ACGT"
      seed: null

  - id: "synthetic_medium_dataset"                  # string: Unique dataset ID
    name: "Medio"              # string: Descriptive name
    type: "synthetic"                 # string: Dataset type
    parameters:                       # Parameters specific to synthetic datasets:
      # Modo de geração: "random" | "noise" | "mutations" | "clustered"
      mode: "random"

      # Parâmetros comuns
      n: 40
      L: 80
      alphabet: "ACGT"
      seed: null

# =====================================================================
# SECTION 3: ALGORITHMS (STANDARDIZED FOR ALL)
# =====================================================================
# Defines algorithm configurations that can be reused.
# Allows creating parameter "presets" for different scenarios.
algorithms:
  - id: "full_config"              # string: Unique configuration ID
    name: "Full Configuration"     # string: Descriptive name
    description: "Algorithms with full parameters"           # string: Detailed description
    algorithms:                       # list: List of algorithms included in this configuration
      - "Baseline"                    # Baseline algorithm (simplest)
      - "BLF-GA"                      # Block-based Genetic Algorithm (main)
      - "CSC"                         # Closest String with Constraints
      - "H2-CSP"                       # Heuristic Closest String Problem
      - "DP-CSP"                      # Dynamic Programming CSP
    algorithm_params:                 # dict: Algorithm parameters (optional - empty dict {} if not needed)
      # Algorithms without parameters listed here will use empty parameters automatically
      # Example with parameters:
      # "BLF-GA":
      #   population_size: 100
      #   generations: 50
                                      
  - id: "small_config"              # string: Unique configuration ID
    name: "Small Configuration"                            # string: Descriptive name
    description: "Small Configuration"      # string: Detailed description
    algorithms:                       # list: List of algorithms included in this configuration
      - "CSC"                         # Closest String with Constraints
      - "H2-CSP"                       # Heuristic Closest String Problem
    algorithm_params: {}              # dict: Empty parameters - algorithms will use default/empty parameters
    
# =====================================================================
# SECTION 4: TASK TYPE (REQUIRED)
# =====================================================================
# Defines the type of operation to be performed.
# Each type has specific configurations in the corresponding section.
task:
  type: "sensitivity"                 # string: Task type
                                      # "experiment" = run algorithms with fixed parameters
                                      # "optimization" = optimize hyperparameters with Optuna
                                      # "sensitivity" = analyze sensitivity with SALib

# =====================================================================
# SECTION 5A: SPECIFIC CONFIGURATION - EXPERIMENT
# =====================================================================
# Use this section when task.type = "experiment"
# Runs algorithms on datasets with pre-defined parameters
experiment:
  tasks:
    
# =====================================================================
# SECTION 5C: SPECIFIC CONFIGURATION - SENSITIVITY
# =====================================================================
# Use this section when task.type = "sensitivity"
# Performs sensitivity analysis of parameters using SALib
sensitivity:
  # Analysis framework
  framework: "SALib"                     # string: Analysis framework (only "SALib" supported)
  
  # List of sensitivity analyses to perform
  tasks:
    - name: "Morris Analysis BLF-GA Small"  # string: Descriptive analysis name
      method: "morris"                # string: Sensitivity analysis method
                                      # "morris" = Morris analysis (elementary effects) - recommended for screening
      
      # Specific datasets and algorithm to analyze
      datasets: 
        - "synthetic_small_dataset"   # list: Dataset IDs to use (reference to section 2)
      algorithm: 
        - "full_config"               # string: Algorithm configuration to analyze

      # Parameters to analyze (defines variation ranges)
      parameters:
        "BLF-GA":
          pop_size:                     # Parameter name
            type: "integer"             # string: Parameter type
            bounds: [10, 50]            # list: [min, max] for int/float
            default: 30                 # default value for reference
            
          max_gens:
            type: "integer"
            bounds: [50, 200]
            default: 100
            
          cross_prob:
            type: "float"               # continuous float
            bounds: [0.6, 0.95]
            default: 0.8
            
          mut_prob:
            type: "float"
            bounds: [0.01, 0.3]
            default: 0.1
            
          crossover_type:
            type: "categorical"         # Discrete values
            values: ["one_point", "uniform", "blend_blocks"]      # list: Options to vary
            default: "one_point"
      
      # Specific configuration for Morris method
      config:
        samples: 10                    # int: Number of samples
        seed: 42
        levels: 4                       # int: Levels for Morris grid (4-10)
        grid_jump: 2                    # int: Grid jump size
        trajectories: 10                # int: Number of trajectories to generate
        optimal_trajectories: null      # int|null: Optimize trajectories (null = don't optimize)
    
    - name: "Sobol Analysis CSC Medium"  # string: Descriptive analysis name
      method: "sobol"                   # string: Sobol analysis for detailed sensitivity
      
      datasets: 
        - "synthetic_medium_dataset"
      algorithm: 
        - "small_config"                # Reference configuration with CSC and H2-CSP
      
      parameters:
        "CSC":
          min_d:
            type: "integer"
            bounds: [1, 4]
            default: 2
          d_factor:
            type: "float"
            bounds: [0.5, 0.9]
            default: 0.75
          min_blocks:
            type: "integer"
            bounds: [2, 6]
            default: 4
          max_blocks:
            type: "integer"
            bounds: [4, 10]
            default: 8
        
        "H2-CSP":
          beam_width:
            type: "integer"
            bounds: [16, 64]
            default: 32
          k_candidates:
            type: "integer"
            bounds: [3, 8]
            default: 5
          local_iters:
            type: "integer"
            bounds: [1, 5]
            default: 3
      
      config:
        samples: 20                    # int: Number of samples for Sobol
        seed: 42
        second_order: true              # bool: Calculate second order indices
        resamples: 500                  # int: Resamples for confidence bootstrap
        confidence_level: 0.95          # float: Confidence level

# =====================================================================
# SECTION 7: RESOURCE SETTINGS (OPTIONAL)
# =====================================================================
# Control of computational resource usage
resources:
  # CPU limitations
  cpu:
    # Do not use the same cores as the main application during tests
    exclusive_cores: true            # bool: If true, tests do not share cores with the main app
    max_workers: null                 # int|null: Maximum number of cores to use (null = all available)
    internal_jobs: 4                  # int: Maximum internal parallel jobs per algorithm
                                      # Controls internal algorithm parallelism (threads/processes)
                                      # Suggested value: 4 for CPUs with 8+ cores
                                      # Relation: max_workers × internal_jobs ≤ CPU_cores
  # Timeouts and limits
  timeouts:
    timeout_per_item: 300       # int: Timeout per individual algorithm execution (seconds)
                                      # Applied to each algorithm run independently
                                      # Common values: 1800 (30min), 3600 (1h), 7200 (2h)
    timeout_total_batch: 21600        # int: Total timeout for entire batch execution (seconds)
                                      # Applied to the complete batch process
                                      # Common values: 21600 (6h), 43200 (12h), 86400 (24h)
    
# =====================================================================
# SECTION 8: SYSTEM SETTINGS (OPTIONAL)
# =====================================================================
# General system configurations and behavior
system:
  # Global seed substitution - overrides all local seeds
  global_seed: null                   # int|null: Global seed that replaces all algorithm and dataset seeds
                                      # When set, substitutes ALL seed parameters throughout configuration
                                      # null = use individual seeds, int = override all with this value
  
  seed_increment: true                # bool: Whether to increment seeds for each repetition
                                      # When true, each repetition uses incremented seeds:
                                      # - If global_seed is set: repetition_1: global_seed+1, repetition_2: global_seed+2, etc.
                                      # - If global_seed is null: increments individual algorithm seeds
                                      # This ensures different seeds while maintaining reproducibility
                                      # false = all repetitions use the same seeds
  
  # Distance calculation method
  distance_method: "hamming"          # string: Method for distance calculation
                                      # "hamming" = Hamming distance (current implementation)
                                      # Future: "levenshtein", "edit", "custom"
  
  # Distance calculation cache
  enable_distance_cache: true         # bool: Enable caching for distance calculations
                                      # true = cache results for better performance
                                      # false = calculate distances every time (uses less memory)