#!/usr/bin/env python3
"""
Ponto de entrada para execu√ß√£o BLFGA automatizada.
"""

import os
import sys

# ensure project root is in path for module imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir)))

import argparse
import resource
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from functools import reduce
from itertools import product
from multiprocessing import cpu_count
from operator import mul
from typing import Any

import pandas as pd
import psutil
import yaml
from datasets.dataset_synthetic import generate_dataset_with_params

from algorithms.blf_ga.algorithm import BLFGAAlgorithm
from algorithms.blf_ga.config import BLF_GA_DEFAULTS

# Configura√ß√µes
CONFIG_DIR = os.path.join(os.path.dirname(__file__), "configs")
RESULTS_DIR = os.path.join(os.path.dirname(__file__), "results")
os.makedirs(RESULTS_DIR, exist_ok=True)


@dataclass
class ExperimentTask:
    """Representa uma tarefa de experimento para execu√ß√£o paralela."""

    dataset_idx: int
    exp_idx: int
    strings: list[str]
    alphabet: str
    params: dict[str, Any]
    dataset_info: dict[str, Any]


@dataclass
class ExperimentResult:
    """Resultado de um experimento."""

    dataset_idx: int
    exp_idx: int
    dataset_n: int
    dataset_L: int
    params: dict[str, Any]
    dist: float
    tempo: float
    memoria_usada: float
    success: bool
    error_msg: str = ""


def load_yaml(path: str) -> dict:
    """Carrega arquivo YAML."""
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f)


def limit_worker_memory(max_mem_gb: float = 1.0):
    """Limita mem√≥ria de um worker espec√≠fico."""
    try:
        _soft, hard = resource.getrlimit(resource.RLIMIT_AS)
        max_bytes = int(max_mem_gb * 1024**3)
        resource.setrlimit(resource.RLIMIT_AS, (max_bytes, hard))
    except Exception as e:
        print(f"[Warning] N√£o foi poss√≠vel limitar mem√≥ria do worker: {e}")


def execute_single_experiment(task: ExperimentTask) -> ExperimentResult:
    """
    Executa um √∫nico experimento BLF-GA.

    Args:
        task: Tarefa contendo todos os dados necess√°rios

    Returns:
        ExperimentResult: Resultado do experimento
    """
    # Obter limite de mem√≥ria dos argumentos ou usar padr√£o
    memory_limit = float(os.environ.get("BLFGA_MEMORY_LIMIT", "1.0"))

    # Limitar mem√≥ria do worker
    limit_worker_memory(max_mem_gb=memory_limit)

    try:
        # Monitorar mem√≥ria inicial
        process = psutil.Process(os.getpid())
        mem_before = process.memory_info().rss / 1024**2

        # Executar algoritmo
        alg = BLFGAAlgorithm(task.strings, task.alphabet, **task.params)
        t0 = time.time()
        _, dist, _ = alg.run_with_history()
        t1 = time.time()

        # Monitorar mem√≥ria final
        mem_after = process.memory_info().rss / 1024**2
        memoria_usada = mem_after - mem_before

        return ExperimentResult(
            dataset_idx=task.dataset_idx,
            exp_idx=task.exp_idx,
            dataset_n=len(task.strings),
            dataset_L=len(task.strings[0]),
            params=task.params.copy(),
            dist=dist,
            tempo=t1 - t0,
            memoria_usada=memoria_usada,
            success=True,
        )

    except Exception as e:  # pylint: disable=broad-except
        return ExperimentResult(
            dataset_idx=task.dataset_idx,
            exp_idx=task.exp_idx,
            dataset_n=len(task.strings) if task.strings else 0,
            dataset_L=len(task.strings[0]) if task.strings else 0,
            params=task.params.copy(),
            dist=float("inf"),
            tempo=0.0,
            memoria_usada=0.0,
            success=False,
            error_msg=str(e),
        )


def calculate_optimal_workers(
    total_experiments: int,
    available_memory_gb: float,
    user_workers: int | None = None,
) -> int:
    """
    Calcula o n√∫mero √≥timo de workers baseado em recursos dispon√≠veis.

    Args:
        total_experiments: N√∫mero total de experimentos
        available_memory_gb: Mem√≥ria dispon√≠vel em GB
        user_workers: N√∫mero de workers especificado pelo usu√°rio (opcional)

    Returns:
        int: N√∫mero √≥timo de workers
    """
    # Se o usu√°rio especificou um n√∫mero, use-o (com valida√ß√£o b√°sica)
    if user_workers is not None:
        if user_workers <= 0:
            print(f"‚ö†Ô∏è N√∫mero de workers inv√°lido ({user_workers}). Usando padr√£o.")
        elif user_workers > cpu_count() * 2:
            print(f"‚ö†Ô∏è Muitos workers ({user_workers}). Limitando a {cpu_count() * 2}.")
            return min(user_workers, cpu_count() * 2)
        else:
            print(f"üë§ Usando {user_workers} workers (especificado pelo usu√°rio)")
            return user_workers

    # Limites baseados em recursos (padr√£o: usar todos os CPUs dispon√≠veis)
    max_workers_cpu = cpu_count()  # Usar todos os CPUs por padr√£o
    max_workers_memory = max(1, int(available_memory_gb // 1.5))  # 1.5GB por worker
    max_workers_experiments = min(total_experiments, 16)  # M√°ximo 16 workers

    optimal = min(max_workers_cpu, max_workers_memory, max_workers_experiments)
    return max(1, optimal)


def parse_arguments():
    """Parse argumentos de linha de comando."""
    parser = argparse.ArgumentParser(
        description="Execu√ß√£o paralela do BLF-GA para m√∫ltiplos experimentos",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  python run_blfga_parallel.py                    # Usar todos os CPUs
  python run_blfga_parallel.py --workers 4        # Usar 4 workers
  python run_blfga_parallel.py --workers auto     # Detec√ß√£o autom√°tica (padr√£o)

Vari√°veis de ambiente:
  BLFGA_WORKERS=4    # Definir n√∫mero de workers via env var
        """,
    )

    parser.add_argument(
        "--workers",
        "-w",
        type=str,
        default=None,
        help='N√∫mero de workers paralelos. Use "auto" para detec√ß√£o autom√°tica, ou um n√∫mero espec√≠fico (padr√£o: n√∫mero de CPUs)',
    )

    parser.add_argument(
        "--memory-limit",
        type=float,
        default=1.0,
        help="Limite de mem√≥ria por worker em GB (padr√£o: 1.0)",
    )

    parser.add_argument(
        "--max-experiments",
        type=int,
        default=1000,
        help="Limite m√°ximo de experimentos (padr√£o: 1000)",
    )

    return parser.parse_args()


def get_worker_count(args) -> int | None:
    """Determina o n√∫mero de workers baseado em argumentos e vari√°veis de ambiente."""

    # 1. Verificar argumento de linha de comando
    if args.workers is not None:
        if args.workers.lower() == "auto":
            return None  # Usar detec√ß√£o autom√°tica
        try:
            return int(args.workers)
        except ValueError:
            print(f"‚ö†Ô∏è Valor inv√°lido para --workers: '{args.workers}'. Usando detec√ß√£o autom√°tica.")
            return None

    # 2. Verificar vari√°vel de ambiente
    env_workers = os.environ.get("BLFGA_WORKERS")
    if env_workers:
        if env_workers.lower() == "auto":
            return None
        try:
            workers = int(env_workers)
            print(f"üåç Usando {workers} workers (vari√°vel de ambiente BLFGA_WORKERS)")
            return workers
        except ValueError:
            print(f"‚ö†Ô∏è Valor inv√°lido em BLFGA_WORKERS: '{env_workers}'. Usando detec√ß√£o autom√°tica.")

    # 3. Padr√£o: usar detec√ß√£o autom√°tica (todos os CPUs)
    return None


def main():
    """Fun√ß√£o principal com execu√ß√£o paralela."""
    # Parse argumentos
    args = parse_arguments()
    user_workers = get_worker_count(args)

    print(f"[Sistema] PID: {os.getpid()}")
    print(f"[Sistema] CPUs dispon√≠veis: {cpu_count()}")

    # Monitorar recursos do sistema
    memory = psutil.virtual_memory()
    available_memory_gb = memory.available / (1024**3)
    print(f"[Sistema] Mem√≥ria dispon√≠vel: {available_memory_gb:.1f} GB")

    # Carregar configura√ß√µes
    dataset_params_config = load_yaml(os.path.join(CONFIG_DIR, "dataset.yaml"))
    blfga_param_grid = load_yaml(os.path.join(CONFIG_DIR, "blfga_grid.yaml"))
    if blfga_param_grid is None:
        blfga_param_grid = {}

    # Gerar combina√ß√µes de datasets
    dataset_param_names = list(dataset_params_config.keys())
    dataset_param_values = [val if isinstance(val, list) else [val] for val in dataset_params_config.values()]
    dataset_configs = [dict(zip(dataset_param_names, values)) for values in product(*dataset_param_values)]

    print(f"[Config] Datasets: {len(dataset_configs)}")
    print(
        f"[Config] Par√¢metros: n={dataset_params_config.get('n')}, "
        f"L={dataset_params_config.get('L')}, "
        f"alphabet='{dataset_params_config.get('alphabet')}'"
    )

    # Preparar tarefas de experimentos
    all_tasks = []
    total_experiments = 0

    for ds_idx, ds_params in enumerate(dataset_configs):
        # Gerar dataset
        strings, params_usados = generate_dataset_with_params(ds_params)
        print(f"[Dataset {ds_idx+1}] n={len(strings)}, L={len(strings[0])}, " f"|Œ£|={len(params_usados['alphabet'])}")

        # Preparar grid BLF-GA
        param_names = list(BLF_GA_DEFAULTS.keys())
        param_values = [
            (
                blfga_param_grid.get(k, [BLF_GA_DEFAULTS[k]])
                if isinstance(blfga_param_grid.get(k, BLF_GA_DEFAULTS[k]), list)
                else [blfga_param_grid.get(k, BLF_GA_DEFAULTS[k])]
            )
            for k in param_names
        ]

        # Calcular experimentos para este dataset
        dataset_experiments = reduce(mul, [len(v) for v in param_values], 1)
        total_experiments += dataset_experiments

        print(f"[Dataset {ds_idx+1}] Experimentos: {dataset_experiments}")

        # Criar tarefas
        for exp_idx, valores in enumerate(product(*param_values)):
            params = dict(zip(param_names, valores))
            task = ExperimentTask(
                dataset_idx=ds_idx,
                exp_idx=exp_idx,
                strings=strings,
                alphabet=params_usados["alphabet"],
                params=params,
                dataset_info=params_usados,
            )
            all_tasks.append(task)

    print(f"[Total] Experimentos: {total_experiments}")

    # Verificar limites de seguran√ßa
    max_experiments_limit = args.max_experiments
    if total_experiments > max_experiments_limit:
        print(
            f"‚ùå N√∫mero de experimentos ({total_experiments}) excede o limite seguro ({max_experiments_limit}). "
            "Reduza o grid de par√¢metros ou use --max-experiments."
        )
        return

    # Calcular workers √≥timos
    optimal_workers = calculate_optimal_workers(total_experiments, available_memory_gb, user_workers)
    print(f"[Paraleliza√ß√£o] Workers utilizados: {optimal_workers}")

    # Informa√ß√µes adicionais sobre a configura√ß√£o
    efficiency = total_experiments / optimal_workers if optimal_workers > 0 else 0
    print(f"[Paraleliza√ß√£o] Experimentos por worker: {efficiency:.1f}")
    print(f"[Paraleliza√ß√£o] Mem√≥ria estimada total: {optimal_workers * args.memory_limit:.1f} GB")

    # Executar experimentos em paralelo
    all_results = []
    start_time = time.time()

    # Configurar vari√°vel de ambiente para workers
    os.environ["BLFGA_MEMORY_LIMIT"] = str(args.memory_limit)

    try:
        # Importar tqdm se dispon√≠vel para progress bar
        try:
            from tqdm import tqdm

            progress_bar = tqdm(total=total_experiments, desc="Experimentos")
        except ImportError:
            progress_bar = None
            print("[Info] tqdm n√£o dispon√≠vel. Instale com: pip install tqdm")

        with ProcessPoolExecutor(max_workers=optimal_workers) as executor:
            # Submeter todas as tarefas
            future_to_task = {executor.submit(execute_single_experiment, task): task for task in all_tasks}

            # Coletar resultados conforme completam
            for future in as_completed(future_to_task):
                result = future.result()
                all_results.append(result)

                if progress_bar:
                    progress_bar.update(1)
                else:
                    print(f"[Progresso] {len(all_results)}/{total_experiments} conclu√≠dos")

                if not result.success:
                    print(f"‚ùå Erro no experimento DS{result.dataset_idx+1}-{result.exp_idx+1}: " f"{result.error_msg}")

        if progress_bar:
            progress_bar.close()

    except KeyboardInterrupt:
        print("\n‚ùå Execu√ß√£o interrompida pelo usu√°rio!")
        return
    except Exception as e:  # pylint: disable=broad-except
        print(f"‚ùå Erro na execu√ß√£o paralela: {e}")
        return

    total_time = time.time() - start_time
    print(f"\n‚úÖ Execu√ß√£o conclu√≠da em {total_time:.1f}s")

    # Processar e salvar resultados
    if all_results:
        successful_results = [r for r in all_results if r.success]
        failed_results = [r for r in all_results if not r.success]

        print(f"[Resultados] Sucessos: {len(successful_results)}, " f"Falhas: {len(failed_results)}")

        if successful_results:
            # Converter para DataFrame
            result_data = []
            for result in successful_results:
                row = {
                    "dataset_idx": result.dataset_idx,
                    "exp_idx": result.exp_idx,
                    "dataset_n": result.dataset_n,
                    "dataset_L": result.dataset_L,
                    **result.params,
                    "dist": result.dist,
                    "tempo": result.tempo,
                    "memoria_mb": result.memoria_usada,
                }
                result_data.append(row)

            df = pd.DataFrame(result_data)

            # Salvar resultados
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            csv_path = os.path.join(RESULTS_DIR, f"blfga_parallel_{timestamp}.csv")
            df.to_csv(csv_path, index=False)

            # Estat√≠sticas
            print("\nüìä Estat√≠sticas:")
            print(f"   Tempo m√©dio por experimento: {df['tempo'].mean():.2f}s")
            print(f"   Melhor dist√¢ncia encontrada: {df['dist'].min()}")
            print(f"   Mem√≥ria m√©dia por experimento: {df['memoria_mb'].mean():.1f} MB")
            print(f"   Speedup estimado: {len(successful_results) * df['tempo'].mean() / total_time:.1f}x")

            print(f"\nüíæ Resultados salvos em: {csv_path}")

        if failed_results:
            print(f"\n‚ö†Ô∏è {len(failed_results)} experimentos falharam")
            error_summary = {}
            for result in failed_results:
                error_type = type(Exception(result.error_msg)).__name__
                error_summary[error_type] = error_summary.get(error_type, 0) + 1
            for error_type, count in error_summary.items():
                print(f"   {error_type}: {count} ocorr√™ncias")
    else:
        print("‚ùå Nenhum resultado foi gerado.")


if __name__ == "__main__":
    main()
