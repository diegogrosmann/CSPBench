"""
Orquestrador de Execu√ß√£o

Concentra toda a l√≥gica de execu√ß√£o de algoritmos CSP,
tanto para execu√ß√µes √∫nicas quanto batches.
"""

import json
import os
import time
import uuid
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from multiprocessing import cpu_count
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from src.domain import Dataset
from src.domain.errors import AlgorithmExecutionError
from src.infrastructure.logging_config import get_logger
from src.infrastructure.orchestrators.base_orchestrator import BaseOrchestrator


class ExecutionOrchestrator(BaseOrchestrator):
    """Orquestrador respons√°vel pela execu√ß√£o de algoritmos CSP."""

    def __init__(self, algorithm_registry, dataset_repository, monitoring_service=None):
        """
        Inicializa orquestrador de execu√ß√£o.

        Args:
            algorithm_registry: Registry de algoritmos
            dataset_repository: Reposit√≥rio de datasets
            monitoring_service: Servi√ßo de monitoramento opcional
        """
        super().__init__(monitoring_service)
        self._algorithm_registry = algorithm_registry
        self._dataset_repository = dataset_repository
        self._executions: Dict[str, Dict[str, Any]] = {}
        self._current_batch_config: Optional[Dict[str, Any]] = None
        self._partial_results_file: Optional[str] = None
        self._logger = get_logger(__name__)

    def execute(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Implementa m√©todo abstrato do BaseOrchestrator.

        Args:
            config: Configura√ß√£o da execu√ß√£o

        Returns:
            Dict[str, Any]: Resultado da execu√ß√£o
        """
        # Implementa√ß√£o padr√£o que delega para execute_batch
        results = self.execute_batch(config, self.monitoring_service)
        return {"results": results}

    def set_batch_config(self, batch_config: Dict[str, Any]) -> None:
        """Define configura√ß√£o do batch atual."""
        self._current_batch_config = batch_config
        self._logger.debug(f"Configura√ß√£o de batch definida: {type(batch_config)}")

        # Configurar salvamento parcial se habilitado
        if self._should_save_partial_results():
            self._setup_partial_results_file()

    def execute_single(
        self,
        algorithm_name: str,
        dataset: Dataset,
        params: Optional[Dict[str, Any]] = None,
        timeout: Optional[int] = None,
        monitoring_service=None,
    ) -> Dict[str, Any]:
        """
        Executa um algoritmo √∫nico.

        Args:
            algorithm_name: Nome do algoritmo a executar
            dataset: Dataset para processamento
            params: Par√¢metros espec√≠ficos do algoritmo
            timeout: Timeout em segundos
            monitoring_service: Servi√ßo de monitoramento opcional

        Returns:
            Dict[str, Any]: Resultado da execu√ß√£o
        """
        from algorithms import global_registry

        # Verifica se algoritmo existe
        if algorithm_name not in global_registry:
            raise AlgorithmExecutionError(
                f"Algoritmo '{algorithm_name}' n√£o encontrado"
            )

        algorithm_class = global_registry[algorithm_name]
        params = params or {}

        # Aplicar configura√ß√µes de infraestrutura se batch_config dispon√≠vel
        if self._current_batch_config:
            infrastructure_config = self._current_batch_config.get("infrastructure", {})
            history_config = infrastructure_config.get("history", {})

            # Injetar par√¢metros de hist√≥rico se habilitados
            if history_config.get("save_history", False):
                params = params.copy()  # N√£o modificar o original
                params["save_history"] = True
                params["history_frequency"] = history_config.get("history_frequency", 1)

        # Cria identificador √∫nico para execu√ß√£o
        execution_id = str(uuid.uuid4())

        try:
            # Registra in√≠cio da execu√ß√£o
            start_time = time.time()
            self._executions[execution_id] = {
                "status": "running",
                "algorithm": algorithm_name,
                "start_time": start_time,
                "params": params.copy(),
            }

            # Instancia e executa algoritmo
            algorithm = algorithm_class(
                strings=dataset.sequences, alphabet=dataset.alphabet, **params
            )

            # Configurar callback de progresso se fornecido
            if monitoring_service:

                def progress_callback(message: str):
                    # Usando algorithm_callback da MonitoringInterface
                    monitoring_service.algorithm_callback(
                        algorithm_name=algorithm_name,
                        progress=0.5,  # Progresso gen√©rico, algoritmo pode n√£o informar progresso espec√≠fico
                        message=message,
                        item_id=execution_id,
                    )

                algorithm.set_progress_callback(progress_callback)

            # Executa algoritmo
            best_string, max_distance, metadata = algorithm.run()
            end_time = time.time()

            # Constroi resultado
            result = {
                "algorithm": algorithm_name,
                "best_string": best_string,
                "max_distance": max_distance,
                "execution_time": end_time - start_time,
                "execution_id": execution_id,
                "params": params,
                "metadata": metadata,
                "dataset": {
                    "size": len(dataset.sequences),
                    "length": len(dataset.sequences[0]) if dataset.sequences else 0,
                    "alphabet": dataset.alphabet,
                },
                "status": "completed",
            }

            # Atualiza status
            self._executions[execution_id].update(
                {"status": "completed", "result": result, "end_time": end_time}
            )

            return result

        except Exception as e:
            # Registra erro
            self._executions[execution_id].update(
                {"status": "failed", "error": str(e), "end_time": time.time()}
            )
            raise AlgorithmExecutionError(
                f"Erro na execu√ß√£o de '{algorithm_name}': {e}"
            )

    def execute_batch(
        self, batch_config: Dict[str, Any], monitoring_service=None
    ) -> List[Dict[str, Any]]:
        """
        Executa batch de algoritmos.

        Args:
            batch_config: Configura√ß√£o do batch
            monitoring_service: Servi√ßo de monitoramento opcional

        Returns:
            List[Dict[str, Any]]: Lista de resultados da execu√ß√£o
        """
        self.set_batch_config(batch_config)

        # Detectar tipo de batch
        task_type_str = batch_config.get("task", {}).get("type", "execution")

        # Determinar tipo de monitoramento
        if monitoring_service:
            from src.presentation.monitoring.interfaces import TaskType

            task_type = getattr(TaskType, task_type_str.upper(), TaskType.EXECUTION)
            monitoring_service.start_monitoring(task_type, batch_config)

        results = []

        self._logger.debug(f"Task type detectado: {task_type_str}")

        if task_type_str == "execution" and "task" in batch_config:
            results = self._execute_structured_batch(batch_config, monitoring_service)
        elif "experiments" in batch_config:
            results = self._execute_legacy_batch(batch_config, monitoring_service)
        else:
            # Estrutura simples - compatibilidade
            algorithms = batch_config.get("algorithms", [])
            datasets = batch_config.get("datasets", [])
            default_params = batch_config.get("params", {})

            for algorithm_name in algorithms:
                for dataset in datasets:
                    try:
                        result = self.execute_single(
                            algorithm_name=algorithm_name,
                            dataset=dataset,
                            params=default_params.get(algorithm_name, {}),
                        )
                        results.append(result)

                        # Salvar resultado parcial se habilitado
                        if self._should_save_partial_results():
                            self._save_partial_result(result)

                    except Exception as e:
                        error_result = {
                            "algorithm": algorithm_name,
                            "dataset": getattr(dataset, "name", "unknown"),
                            "status": "failed",
                            "error": str(e),
                        }
                        results.append(error_result)

                        # Salvar resultado de erro parcial se habilitado
                        if self._should_save_partial_results():
                            self._save_partial_result(error_result)

        return results

    def _execute_structured_batch(
        self, batch_config: Dict[str, Any], monitoring_service=None
    ) -> List[Dict[str, Any]]:
        """Executa batch com estrutura nova (datasets + algorithms + executions)."""
        self._logger.debug("Processando batch de execu√ß√£o estruturado")

        # Nova estrutura com datasets e algoritmos por ID
        datasets_config = batch_config.get("datasets", [])
        algorithms_config = batch_config.get("algorithms", [])
        executions = batch_config["execution"]["executions"]

        self._logger.debug(f"Datasets: {len(datasets_config)}")
        self._logger.debug(f"Algorithms: {len(algorithms_config)}")
        self._logger.debug(f"Executions: {len(executions)}")

        # Inicializar dados de monitoramento
        if monitoring_service:
            self._setup_monitoring_data(
                executions, datasets_config, algorithms_config, monitoring_service
            )

        from src.infrastructure import FileDatasetRepository

        dataset_repo = FileDatasetRepository("./datasets")

        results = []
        execution_index = 0

        for execution in executions:
            execution_index += 1
            execution_name = execution.get("nome", f"Execu√ß√£o {execution_index}")

            # Iterar sobre configura√ß√µes de algoritmo
            algorithm_ids = execution["algorithms"]
            for algo_config_idx, algorithm_id in enumerate(algorithm_ids, 1):

                # Resolver configura√ß√£o do algoritmo
                algorithm_config = next(
                    (a for a in algorithms_config if a["id"] == algorithm_id), None
                )
                if not algorithm_config:
                    self._logger.error(
                        f"Algoritmo com ID '{algorithm_id}' n√£o encontrado"
                    )
                    continue

                # Resolver datasets para esta configura√ß√£o
                dataset_ids = execution["datasets"]
                for dataset_idx, dataset_id in enumerate(dataset_ids, 1):

                    dataset_config = next(
                        (d for d in datasets_config if d["id"] == dataset_id), None
                    )
                    if not dataset_config:
                        results.append(
                            {
                                "execution_name": execution.get("nome", "unknown"),
                                "dataset_id": dataset_id,
                                "status": "error",
                                "error": f"Dataset com ID '{dataset_id}' n√£o encontrado",
                            }
                        )
                        continue

                    # Atualizar informa√ß√µes do dataset no monitoramento
                    if monitoring_service:
                        # Contar algoritmos √∫nicos desta configura√ß√£o
                        unique_algorithms = set(algorithm_config["algorithms"])

                        # Atualizar hierarquia de dataset (que j√° inclui execu√ß√£o)
                        from src.presentation.monitoring.interfaces import (
                            ExecutionLevel,
                        )

                        # Obter nome do dataset
                        dataset_name = dataset_config.get("nome", dataset_id)

                        # Obter nome da configura√ß√£o de algoritmo
                        algorithm_config_name = algorithm_config.get(
                            "nome", "Algoritmos"
                        )

                        monitoring_service.monitor.update_hierarchy(
                            level=ExecutionLevel.DATASET,
                            level_id=f"{dataset_id}_{algorithm_id}",
                            progress=0.0,
                            message=f"Processando dataset {dataset_name}",
                            data={
                                "execution_name": execution_name,
                                "config_index": execution_index,
                                "total_configs": len(executions),
                                "dataset_name": dataset_name,
                                "dataset_index": dataset_idx,
                                "total_datasets": len(dataset_ids),
                                "algorithm_config_name": algorithm_config_name,
                                "algorithm_config_index": algo_config_idx,
                                "total_algorithm_configs": len(algorithm_ids),
                                "total_algorithms": len(unique_algorithms),
                            },
                        )

                    # Carregar dataset e executar algoritmos desta configura√ß√£o
                    dataset_results = self._execute_dataset_algorithms_for_config(
                        execution,
                        dataset_config,
                        dataset_id,
                        dataset_repo,
                        algorithm_config,
                        monitoring_service,
                    )
                    results.extend(dataset_results)

            # Configura√ß√µes completadas s√£o controladas pela hierarquia
            # N√£o precisamos mais usar update_execution_data

        return results

    def _execute_legacy_batch(
        self, batch_config: Dict[str, Any], monitoring_service=None
    ) -> List[Dict[str, Any]]:
        """Executa batch com estrutura legada (experiments)."""
        from src.infrastructure import FileDatasetRepository

        dataset_repo = FileDatasetRepository("./datasets")

        results = []
        for exp in batch_config.get("experiments", []):
            try:
                print(f"[DEBUG] Processando experimento: {exp}")
                # Carrega dataset
                dataset = dataset_repo.load(exp["dataset"])
                print(f"[DEBUG] Dataset carregado: {len(dataset.sequences)} sequ√™ncias")

                result = self.execute_single(
                    algorithm_name=exp["algorithm"],
                    dataset=dataset,
                    params=exp.get("params", {}),
                )
                result["dataset"] = exp["dataset"]
                results.append(result)
                print(f"[DEBUG] Resultado adicionado com sucesso")

                # Salvar resultado parcial se habilitado
                if self._should_save_partial_results():
                    self._save_partial_result(result)

            except Exception as e:
                print(f"[DEBUG] Erro no experimento: {e}")
                error_result = {
                    "algorithm": exp["algorithm"],
                    "dataset": exp["dataset"],
                    "status": "error",
                    "error": str(e),
                }
                results.append(error_result)

                # Salvar resultado de erro parcial se habilitado
                if self._should_save_partial_results():
                    self._save_partial_result(error_result)

        return results

    def _should_save_partial_results(self) -> bool:
        """Verifica se deve salvar resultados parciais."""
        if not self._current_batch_config:
            return False

        infrastructure = self._current_batch_config.get("infrastructure", {})
        result_config = infrastructure.get("result", {})
        return result_config.get("save_partial_results", False)

    def _setup_partial_results_file(self) -> None:
        """Configura arquivo para salvamento de resultados parciais."""
        from src.infrastructure import SessionManager

        try:
            session_manager = SessionManager(self._current_batch_config or {})
            session_folder = session_manager.create_session()
            results_dir = Path(session_manager.get_result_dir())
            print(f"üìÅ Sess√£o criada: {session_folder}")
            print(f"üìÅ Salvando resultados parciais em: {results_dir}")
        except Exception as e:
            # Fallback para diret√≥rio padr√£o
            base_dir = Path("./outputs/results")
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            results_dir = base_dir / timestamp
            print(f"üìÅ Usando diret√≥rio fallback: {results_dir}, erro: {e}")

        results_dir.mkdir(parents=True, exist_ok=True)
        self._partial_results_file = str(results_dir / "partial_results.json")

        print(f"üíæ Arquivo de resultados parciais: {self._partial_results_file}")

        # Inicializar arquivo com array vazio
        with open(self._partial_results_file, "w", encoding="utf-8") as f:
            json.dump([], f)

        print(f"‚úÖ Sistema de salvamento parcial inicializado")

    def _save_partial_result(self, result: Dict[str, Any]) -> None:
        """Salva um resultado parcial no arquivo."""
        if not self._partial_results_file:
            return

        try:
            # Carregar resultados existentes
            if os.path.exists(self._partial_results_file):
                with open(self._partial_results_file, "r", encoding="utf-8") as f:
                    existing_results = json.load(f)
            else:
                existing_results = []

            # Adicionar novo resultado
            existing_results.append(result)

            # Salvar de volta
            with open(self._partial_results_file, "w", encoding="utf-8") as f:
                json.dump(existing_results, f, indent=2, ensure_ascii=False)

            print(
                f"üíæ Resultado salvo [{len(existing_results)}]: {result.get('algorithm', 'N/A')} - {result.get('status', 'N/A')}"
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao salvar resultado parcial: {e}")

    # M√©todos auxiliares para organiza√ß√£o do c√≥digo...
    def _setup_monitoring_data(
        self, executions, datasets_config, algorithms_config, monitoring_service
    ):
        """Configura dados iniciais de monitoramento."""
        # Calcular totais de datasets considerando todas as execu√ß√µes
        total_dataset_executions = 0
        for execution in executions:
            total_dataset_executions += len(execution.get("datasets", []))

        # Contar total de algoritmos √∫nicos
        unique_algorithms = set()
        for execution in executions:
            algorithm_ids = execution["algorithms"]
            for algorithm_id in algorithm_ids:
                algorithm_config = next(
                    (a for a in algorithms_config if a["id"] == algorithm_id), None
                )
                if algorithm_config:
                    algorithms_list = algorithm_config["algorithms"]
                    unique_algorithms.update(algorithms_list)

        total_algorithms = len(unique_algorithms)

        # Dados iniciais configurados, mas n√£o usamos mais update_execution_data
        # O monitoramento agora √© feito atrav√©s de update_hierarchy

        # For√ßar primeira atualiza√ß√£o para mostrar interface
        time.sleep(0.1)

    def _update_execution_monitoring(
        self,
        execution,
        execution_index,
        execution_name,
        datasets_config,
        algorithms_config,
        monitoring_service,
    ):
        """Atualiza dados de monitoramento para execu√ß√£o atual."""
        # Contar algoritmos √∫nicos nesta execu√ß√£o
        execution_algorithms = set()
        for algo_id in execution["algorithms"]:
            algo_config = next(
                (a for a in algorithms_config if a["id"] == algo_id), None
            )
            if algo_config:
                execution_algorithms.update(algo_config["algorithms"])

        # Atualizar hierarquia de execu√ß√£o
        from src.presentation.monitoring.interfaces import ExecutionLevel

        # Obter total de execu√ß√µes do contexto (ser√° passado pelo orchestrator)
        # Por enquanto, usando o index como fallback
        total_executions = execution_index  # Isso ser√° melhorado no orchestrator

        monitoring_service.monitor.update_hierarchy(
            level=ExecutionLevel.EXECUTION,
            level_id=execution_name,
            progress=0.0,
            message=f"Iniciando execu√ß√£o {execution_name}",
            data={
                "execution_name": execution_name,
                "config_index": execution_index,
                "total_configs": total_executions,
            },
        )

    def _execute_dataset_algorithms_for_config(
        self,
        execution,
        dataset_config,
        dataset_id,
        dataset_repo,
        algorithm_config,
        monitoring_service,
    ):
        """Executa algoritmos de uma configura√ß√£o espec√≠fica para um dataset."""
        results = []

        try:
            # Carregar dataset
            if dataset_config["tipo"] == "file":
                filename = dataset_config["parametros"]["filename"]
                dataset = dataset_repo.load(filename)
            else:
                # Para datasets sint√©ticos, criar usando gerador
                dataset = self._create_dataset_from_config(dataset_config)

            self._logger.info(
                f"Dataset {dataset_id} carregado: {len(dataset.sequences)} sequ√™ncias"
            )

            # Executar algoritmos desta configura√ß√£o
            algorithm_names = algorithm_config["algorithms"]
            algorithm_params = algorithm_config.get("algorithm_params", {})
            repetitions = execution.get("repetitions", 1)

            for algorithm_name in algorithm_names:
                # Obter par√¢metros espec√≠ficos do algoritmo
                params = algorithm_params.get(algorithm_name, {})

                # Executar repeti√ß√µes com paralelismo
                algorithm_results = self._execute_algorithm_repetitions_parallel(
                    algorithm_name=algorithm_name,
                    dataset=dataset,
                    params=params,
                    repetitions=repetitions,
                    execution_context={
                        "execution_name": execution.get("nome", "unknown"),
                        "dataset_id": dataset_id,
                        "algorithm_id": algorithm_config["id"],
                    },
                    monitoring_service=monitoring_service,
                )

                results.extend(algorithm_results)

        except Exception as e:
            self._logger.error(
                f"Erro no carregamento/processamento do dataset {dataset_id}: {e}"
            )
            results.append(
                {
                    "execution_name": execution.get("nome", "unknown"),
                    "dataset_id": dataset_id,
                    "status": "error",
                    "error": str(e),
                    "execution_time": 0.0,
                }
            )

        return results

    def _execute_single_repetition(
        self,
        algorithm_name: str,
        dataset: Dataset,
        params: Dict[str, Any],
        execution_context: Dict[str, Any],
        rep_number: int,
        total_repetitions: int,
    ) -> Dict[str, Any]:
        """
        Executa uma √∫nica repeti√ß√£o de um algoritmo.

        Este m√©todo √© projetado para ser usado com ProcessPoolExecutor,
        portanto deve ser independente de estado do orchestrator.

        Args:
            algorithm_name: Nome do algoritmo a executar
            dataset: Dataset para processamento
            params: Par√¢metros do algoritmo
            execution_context: Contexto da execu√ß√£o (nomes, IDs, etc.)
            rep_number: N√∫mero da repeti√ß√£o (1-based)
            total_repetitions: Total de repeti√ß√µes

        Returns:
            Dict[str, Any]: Resultado da execu√ß√£o com contexto
        """
        try:
            # Executar algoritmo (sem monitoring_service pois n√£o √© thread-safe)
            result = self.execute_single(
                algorithm_name, dataset, params, monitoring_service=None
            )

            # Adicionar informa√ß√µes de contexto
            result.update(
                {
                    "execution_name": execution_context.get(
                        "execution_name", "unknown"
                    ),
                    "dataset_id": execution_context.get("dataset_id", "unknown"),
                    "algorithm_id": execution_context.get("algorithm_id", "unknown"),
                    "algorithm_name": algorithm_name,
                    "repetition": rep_number,
                    "total_repetitions": total_repetitions,
                    "status": "success",
                }
            )

            return result

        except Exception as e:
            error_result = {
                "execution_name": execution_context.get("execution_name", "unknown"),
                "dataset_id": execution_context.get("dataset_id", "unknown"),
                "algorithm_id": execution_context.get("algorithm_id", "unknown"),
                "algorithm_name": algorithm_name,
                "repetition": rep_number,
                "total_repetitions": total_repetitions,
                "status": "error",
                "error": str(e),
                "execution_time": 0.0,
            }

            return error_result

    def _get_max_workers(self) -> int:
        """
        Obt√©m o n√∫mero m√°ximo de workers para paraleliza√ß√£o.

        Returns:
            int: N√∫mero de workers a usar
        """
        if self._current_batch_config:
            resources = self._current_batch_config.get("resources", {})
            parallel_config = resources.get("parallel", {})
            max_workers = parallel_config.get("max_workers")

            if max_workers is not None and max_workers > 0:
                return max_workers

        # Fallback para n√∫mero de CPUs
        return cpu_count() or 1

    def _execute_dataset_algorithms(
        self,
        execution,
        dataset_config,
        dataset_id,
        dataset_repo,
        algorithms_config,
        monitoring_service,
    ):
        """Executa algoritmos para um dataset espec√≠fico."""
        results = []

        try:
            # Carregar dataset
            if dataset_config["tipo"] == "file":
                filename = dataset_config["parametros"]["filename"]
                dataset = dataset_repo.load(filename)
            else:
                # Para datasets sint√©ticos, criar usando gerador
                dataset = self._create_dataset_from_config(dataset_config)

            self._logger.info(
                f"Dataset {dataset_id} carregado: {len(dataset.sequences)} sequ√™ncias"
            )

            # Obter configura√ß√µes de algoritmos da execu√ß√£o
            algorithm_ids = execution["algorithms"]
            repetitions = execution.get("repetitions", 1)

            for algorithm_id in algorithm_ids:
                # Resolver configura√ß√£o do algoritmo
                algorithm_config = next(
                    (a for a in algorithms_config if a["id"] == algorithm_id), None
                )
                if not algorithm_config:
                    self._logger.error(
                        f"Algoritmo com ID '{algorithm_id}' n√£o encontrado"
                    )
                    results.append(
                        {
                            "execution_name": execution.get("nome", "unknown"),
                            "dataset_id": dataset_id,
                            "algorithm_id": algorithm_id,
                            "status": "error",
                            "error": f"Algoritmo com ID '{algorithm_id}' n√£o encontrado",
                        }
                    )
                    continue

                # Executar cada algoritmo da configura√ß√£o
                algorithm_names = algorithm_config["algorithms"]
                algorithm_params = algorithm_config.get("algorithm_params", {})

                for algorithm_name in algorithm_names:
                    # Obter par√¢metros espec√≠ficos do algoritmo
                    params = algorithm_params.get(algorithm_name, {})

                    # Executar repeti√ß√µes com paralelismo
                    algorithm_results = self._execute_algorithm_repetitions_parallel(
                        algorithm_name=algorithm_name,
                        dataset=dataset,
                        params=params,
                        repetitions=repetitions,
                        execution_context={
                            "execution_name": execution.get("nome", "unknown"),
                            "dataset_id": dataset_id,
                            "algorithm_id": algorithm_id,
                        },
                        monitoring_service=monitoring_service,
                    )

                    results.extend(algorithm_results)

        except Exception as e:
            self._logger.error(
                f"Erro no carregamento/processamento do dataset {dataset_id}: {e}"
            )
            results.append(
                {
                    "execution_name": execution.get("nome", "unknown"),
                    "dataset_id": dataset_id,
                    "status": "error",
                    "error": f"Erro no dataset: {e}",
                }
            )

        return results

    def _create_dataset_from_config(self, dataset_config: Dict[str, Any]):
        """Cria dataset a partir da configura√ß√£o."""
        from src.domain.dataset import SyntheticDatasetGenerator

        dataset_type = dataset_config["tipo"]
        params = dataset_config.get("parametros", {})

        if dataset_type == "synthetic":
            generator = SyntheticDatasetGenerator()

            # Se h√° par√¢metros de noise, usar generate_from_center
            if "noise" in params and params["noise"] > 0:
                # Gerar string central primeiro
                n = params.get("n", 10)
                L = params.get("L", 20)
                alphabet = params.get("alphabet", "ACTG")
                seed = params.get("seed")

                # Criar string central aleat√≥ria
                import random

                rng = random.Random(seed)
                center = "".join(rng.choice(alphabet) for _ in range(L))

                return generator.generate_from_center(
                    center=center,
                    n=n,
                    noise_rate=params.get("noise", 0.0),
                    alphabet=alphabet,
                    seed=seed,
                )
            else:
                # Usar generate_random para datasets sem ru√≠do
                return generator.generate_random(
                    n=params.get("n", 10),
                    length=params.get("L", 20),
                    alphabet=params.get("alphabet", "ACTG"),
                    seed=params.get("seed"),
                )
        else:
            raise ValueError(
                f"Tipo de dataset '{dataset_type}' n√£o suportado em dataset sint√©tico"
            )

    def get_execution_status(self, execution_id: str) -> str:
        """Obt√©m status de uma execu√ß√£o espec√≠fica."""
        if execution_id not in self._executions:
            return "not_found"
        return self._executions[execution_id]["status"]

    def cancel_execution(self, execution_id: str) -> bool:
        """Cancela uma execu√ß√£o em andamento."""
        if execution_id not in self._executions:
            return False

        execution = self._executions[execution_id]
        if execution["status"] == "running":
            execution["status"] = "cancelled"
            execution["end_time"] = time.time()
            return True

        return False

    def _execute_algorithm_repetitions_parallel(
        self,
        algorithm_name: str,
        dataset: Dataset,
        params: Dict[str, Any],
        repetitions: int,
        execution_context: Dict[str, Any],
        monitoring_service=None,
    ) -> List[Dict[str, Any]]:
        """
        Executa repeti√ß√µes de um algoritmo em paralelo usando ProcessPoolExecutor.

        Args:
            algorithm_name: Nome do algoritmo
            dataset: Dataset para processamento
            params: Par√¢metros do algoritmo
            repetitions: N√∫mero de repeti√ß√µes
            execution_context: Contexto da execu√ß√£o
            monitoring_service: Servi√ßo de monitoramento

        Returns:
            List[Dict[str, Any]]: Lista de resultados das repeti√ß√µes
        """
        max_workers = self._get_max_workers()

        # Se max_workers = 1, usar execu√ß√£o sequencial
        if max_workers == 1:
            return self._execute_algorithm_repetitions_sequential(
                algorithm_name,
                dataset,
                params,
                repetitions,
                execution_context,
                monitoring_service,
            )

        # Execu√ß√£o paralela
        self._logger.debug(
            f"Executando {repetitions} repeti√ß√µes de {algorithm_name} com {max_workers} workers"
        )

        results = []

        # Preparar argumentos para ProcessPoolExecutor
        args_list = []
        for rep in range(repetitions):
            args_list.append(
                (
                    algorithm_name,
                    dataset,
                    params,
                    execution_context,
                    rep + 1,  # 1-based
                    repetitions,
                )
            )

        # Executar em paralelo
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # Submeter todas as tarefas
            future_to_rep = {}
            for i, args in enumerate(args_list):
                future = executor.submit(self._execute_single_repetition, *args)
                future_to_rep[future] = i + 1

            # Coletar resultados conforme completam
            for future in as_completed(future_to_rep):
                rep_number = future_to_rep[future]
                rep_id = f"{algorithm_name}_{execution_context.get('dataset_id', 'unknown')}_{rep_number}"

                try:
                    # Inicializar monitoramento se dispon√≠vel
                    if monitoring_service:
                        from src.presentation.monitoring.interfaces import (
                            HierarchicalContext,
                        )

                        context = HierarchicalContext(
                            dataset_id=execution_context.get("dataset_id", "unknown"),
                            algorithm_id=algorithm_name,
                            repetition_id=f"{rep_number}/{repetitions}",
                        )
                        # Iniciar item antes da execu√ß√£o
                        monitoring_service.start_item(rep_id, "repetition", context)

                    # Obter resultado
                    result = future.result()

                    # Verificar se houve erro
                    if result.get("status") == "error":
                        self._logger.error(
                            f"Erro na execu√ß√£o do algoritmo {algorithm_name} (rep {rep_number}): {result.get('error')}"
                        )

                        # Notificar monitoramento de erro
                        if monitoring_service:
                            monitoring_service.finish_item(
                                rep_id,
                                False,
                                result,
                                result.get("error", "Unknown error"),
                            )
                    else:
                        self._logger.debug(
                            f"Algoritmo {algorithm_name} executado com sucesso (rep {rep_number}/{repetitions})"
                        )

                        # Notificar monitoramento de conclus√£o
                        if monitoring_service:
                            monitoring_service.finish_item(rep_id, True, result)

                    results.append(result)

                except Exception as e:
                    self._logger.error(
                        f"Erro ao processar resultado da repeti√ß√£o {rep_number} de {algorithm_name}: {e}"
                    )

                    error_result = {
                        "execution_name": execution_context.get(
                            "execution_name", "unknown"
                        ),
                        "dataset_id": execution_context.get("dataset_id", "unknown"),
                        "algorithm_id": execution_context.get(
                            "algorithm_id", "unknown"
                        ),
                        "algorithm_name": algorithm_name,
                        "repetition": rep_number,
                        "total_repetitions": repetitions,
                        "status": "error",
                        "error": str(e),
                        "execution_time": 0.0,
                    }

                    results.append(error_result)

                    # Notificar monitoramento de erro
                    if monitoring_service:
                        monitoring_service.finish_item(
                            rep_id, False, error_result, str(e)
                        )

        return results

    def _execute_algorithm_repetitions_sequential(
        self,
        algorithm_name: str,
        dataset: Dataset,
        params: Dict[str, Any],
        repetitions: int,
        execution_context: Dict[str, Any],
        monitoring_service=None,
    ) -> List[Dict[str, Any]]:
        """
        Executa repeti√ß√µes de um algoritmo sequencialmente (fallback).

        Args:
            algorithm_name: Nome do algoritmo
            dataset: Dataset para processamento
            params: Par√¢metros do algoritmo
            repetitions: N√∫mero de repeti√ß√µes
            execution_context: Contexto da execu√ß√£o
            monitoring_service: Servi√ßo de monitoramento

        Returns:
            List[Dict[str, Any]]: Lista de resultados das repeti√ß√µes
        """
        results = []

        for rep in range(repetitions):
            rep_id = f"{algorithm_name}_{execution_context.get('dataset_id', 'unknown')}_{rep+1}"

            try:
                # Notificar monitoramento de novo item
                if monitoring_service:
                    from src.presentation.monitoring.interfaces import (
                        HierarchicalContext,
                    )

                    context = HierarchicalContext(
                        dataset_id=execution_context.get("dataset_id", "unknown"),
                        algorithm_id=algorithm_name,
                        repetition_id=f"{rep+1}/{repetitions}",
                    )
                    # Iniciar item antes da execu√ß√£o
                    monitoring_service.start_item(rep_id, "repetition", context)
                    monitoring_service.update_item(rep_id, 0.0, "Iniciando", context)

                # Executar algoritmo
                result = self.execute_single(
                    algorithm_name, dataset, params, monitoring_service
                )

                # Adicionar informa√ß√µes de contexto
                result.update(
                    {
                        "execution_name": execution_context.get(
                            "execution_name", "unknown"
                        ),
                        "dataset_id": execution_context.get("dataset_id", "unknown"),
                        "algorithm_id": execution_context.get(
                            "algorithm_id", "unknown"
                        ),
                        "algorithm_name": algorithm_name,
                        "repetition": rep + 1,
                        "total_repetitions": repetitions,
                        "status": "success",
                    }
                )

                results.append(result)

                # Notificar monitoramento de conclus√£o
                if monitoring_service:
                    monitoring_service.finish_item(rep_id, True, result)

                self._logger.debug(
                    f"Algoritmo {algorithm_name} executado com sucesso (rep {rep+1}/{repetitions})"
                )

            except Exception as e:
                self._logger.error(
                    f"Erro na execu√ß√£o do algoritmo {algorithm_name} (rep {rep+1}): {e}"
                )

                error_result = {
                    "execution_name": execution_context.get(
                        "execution_name", "unknown"
                    ),
                    "dataset_id": execution_context.get("dataset_id", "unknown"),
                    "algorithm_id": execution_context.get("algorithm_id", "unknown"),
                    "algorithm_name": algorithm_name,
                    "repetition": rep + 1,
                    "total_repetitions": repetitions,
                    "status": "error",
                    "error": str(e),
                    "execution_time": 0.0,
                }

                results.append(error_result)

                # Notificar monitoramento de erro
                if monitoring_service:
                    monitoring_service.finish_item(rep_id, False, error_result, str(e))

        return results
