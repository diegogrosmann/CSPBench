# ===================================================================
# STANDARDIZED TEMPLATE CSPBench - UNIFIED STRUCTURE v0.2
# ===================================================================
# This template defines the standard structure for all batch types.
# Supported types: execution, optimization, sensitivity
# Usage: Copy this template and adapt for your specific needs

# =====================================================================
# SECTION 1: METADATA (REQUIRED FOR ALL)
# =====================================================================
# This section contains basic information about the batch.
# All fields are required for traceability and documentation.
metadados:
  nome: "Batch Name"                      # Descriptive batch name (string)
  descricao: "Detailed description of what the batch does"  # Complete description (string)
  autor: "Author Name"                    # Responsible author (string)
  versao: "1.0"                          # Batch version (string)
  data_criacao: "2025-07-13"             # Creation date in YYYY-MM-DD format
  tags: ["tag1", "tag2"]                 # Tags for categorization (list)
  timeout_global: 3600                   # Global timeout in seconds (int)
                                         # Common values: 3600 (1h), 7200 (2h), 14400 (4h)

# =====================================================================
# SECTION 2: INFRASTRUCTURE SETTINGS (OPTIONAL)
# =====================================================================
# This section allows overriding settings from the settings.yaml file.
# Use only if you need specific settings for this batch.
infrastructure:
  # History system - controls how execution history is saved
  history:
    save_history: true                  # bool: Save detailed history
                                       # true = saves each iteration, false = final result only
    plot_history: true                 # bool: Generate history plots
                                      # true = creates convergence plots, false = no plots
    history_frequency: 1              # int: Save frequency (every N iterations)
                                     # 1 = each iteration, 10 = every 10 iterations
    history_plots:
      plot_convergence: true          # bool: Algorithm convergence plot
      plot_fitness_evolution: true    # bool: Fitness evolution over time
      plot_best_solutions: true       # bool: Timeline of best solutions found
      plot_parameters: true           # bool: Adaptive parameter evolution
      plot_format: "png"              # string: Plot format ("png"|"pdf"|"svg")
  
  # Results system - controls how results are persisted
  result:
    save_partial_results: true        # bool: Save partial results during execution
                                     # true = saves incrementally, false = only at the end
    partial_file: "partial_results.json" # string: Partial results filename 

# =====================================================================
# SECTION 3: DATASETS (STANDARDIZED FOR ALL)
# =====================================================================
# Defines the datasets available for use in the batch.
# Each dataset has a unique ID that can be referenced in executions.
# Supported types: synthetic, file, entrez
datasets:
  # === SYNTHETIC DATASETS ===
  # Generate artificial data for testing and development
  - id: dataset_teste                   # string: Unique dataset ID
    nome: "Test Dataset"                # string: Descriptive name
    tipo: "synthetic"                   # string: Dataset type
    parametros:                         # Parameters specific to synthetic datasets:
      n: 10                            # int: Number of sequences (3-100 recommended)
      L: 20                            # int: Sequence length (5-1000)
      alphabet: "ACGT"                 # string: Sequence alphabet
                                      # "ACGT" = DNA, "01" = binary, "ACDEFGH..." = custom
      noise: 0.1                      # float: Noise level (0.0-1.0)
                                      # 0.0 = no noise, 0.5 = very noisy
      fully_random: false             # bool: Completely random sequences
                                      # false = pattern-based, true = random
      seed: 42                        # int|null: Seed for reproducibility
                                     # int = deterministic, null = random

  # === FILE DATASETS ===
  # Load data from existing FASTA files
  - id: dataset_arquivo                # string: Unique dataset ID
    nome: "File Dataset"               # string: Descriptive name
    tipo: "file"                       # string: Dataset type
    parametros:                        # Parameters specific to file datasets:
      filename: "exemplo.fasta"        # string: Filename (must be in datasets/)
                                      # Supported formats: .fasta, .fa, .txt

  # === ENTREZ DATASETS (NCBI) ===
  # Download data directly from NCBI using Entrez API
  - id: dataset_ncbi                   # string: Unique dataset ID
    nome: "NCBI Dataset"               # string: Descriptive name
    tipo: "entrez"                     # string: Dataset type
    parametros:                        # Parameters specific to Entrez datasets:
      query: "COIGene AND 600:650[SLEN]" # string: NCBI search query
                                         # Syntax: https://www.ncbi.nlm.nih.gov/books/NBK25499/
      db: "nucleotide"                  # string: NCBI database
                                       # "nucleotide", "protein", "pubmed", etc.
      retmax: 10                       # int: Maximum number of sequences to download (1-10000)
                                      # WARNING: High values may be slow

# =====================================================================
# SECTION 4: ALGORITHMS (STANDARDIZED FOR ALL)
# =====================================================================
# Defines algorithm configurations that can be reused.
# Allows creating parameter "presets" for different scenarios.
algorithms:
  - id: "config_padrao"                # string: Unique configuration ID
    nome: "Default Configuration"       # string: Descriptive name
    descricao: "Algorithms with default parameters"  # string: Detailed description
    algorithms:                        # list: List of algorithms included in this configuration
      - "Baseline"                     # Baseline algorithm (simplest)
      - "BLF-GA"                       # Block-based Genetic Algorithm (main)
      - "CSC"                          # Closest String with Constraints
      - "H³-CSP"                       # Heuristic Closest String Problem
      - "DP-CSP"                       # Dynamic Programming CSP
    
    # Algorithm-specific parameters
    # For execution: fixed values used directly
    # For optimization/sensitivity: base values that can be overridden
    algorithm_params:
      # === BASELINE - Simple Greedy Algorithm ===
      "Baseline":
        tie_break: "lex"               # string: Tie-breaking criterion
                                      # "lex" = lexicographic, "random" = random, "first" = first
      
      # === BLF-GA - Block-based Genetic Algorithm ===
      "BLF-GA":
        # Population Parameters
        pop_size: 100                  # int: Population size (50-500 recommended)
        max_gens: 200                  # int: Maximum number of generations (100-1000)
        
        # Genetic Operators
        cross_prob: 0.8                # float: Crossover probability (0.6-0.95)
        mut_prob: 0.1                  # float: Mutation probability (0.01-0.3)
        elite_rate: 0.05               # float: Elitism rate (0.01-0.15)
        
        # Block Parameters (BLF-GA specific)
        initial_blocks: 0.2            # float: Initial block proportion (0.1-0.5)
        rediv_freq: 10                 # int: Redivision frequency (5-25)
        
        # Diversity and Immigration
        immigrant_freq: 15             # int: Immigration frequency (10-30)
        immigrant_ratio: 0.2           # float: Immigration rate (0.1-0.3)
        
        # Operator Types
        crossover_type: "one_point"    # string: Crossover type
                                      # "one_point", "uniform", "blend_blocks"
        mutation_type: "multi"         # string: Mutation type
                                      # "multi", "inversion", "transposition"
        selection_type: "tournament"   # string: Selection type
                                      # "tournament", "roulette", "ranking"
        refinement_type: "greedy"      # string: Local refinement type
                                      # "greedy", "swap", "insertion", "2opt"
        
        # Stopping Criteria
        no_improve_patience: 0.3       # float: Patience without improvement (0.1-0.5)
                                      # Proportion of generations without improvement before stopping
      
      # === CSC - Closest String with Constraints ===
      "CSC":
        min_d: 2                       # int: Initial minimum distance (1-5)
        d_factor: 0.75                 # float: Distance increment factor (0.5-1.0)
        min_blocks: 4                  # int: Minimum number of blocks (2-8)
        max_blocks: 8                  # int: Maximum number of blocks (4-16)
        l_div: 25                      # int: Divisor for block length (10-50)
      
      # === H³-CSP - Heuristic Closest String Problem ===
      "H³-CSP":
        auto_blocks: true              # bool: Automatic block division
                                      # true = automatic, false = manual
        min_block_size: 2              # int: Minimum block size (1-5)
        block_size: 4                  # int: Default block size (2-8)
        k_candidates: 5                # int: Number of candidates considered (3-10)
        local_iters: 3                 # int: Local search iterations (1-5)
        fallback_enabled: true         # bool: Enable fallback for large blocks
      
      # === DP-CSP - Dynamic Programming CSP ===
      "DP-CSP":
        max_d: 5                       # int: Maximum distance considered (3-10)
                                      # High values greatly increase execution time
        warn_threshold: 50             # int: Sequence limit for warning (20-100)
                                      # Above this value, the algorithm may be very slow
        seed: null                     # int|null: Seed for reproducibility

# =====================================================================
# SECTION 5: TASK TYPE (REQUIRED)
# =====================================================================
# Defines the type of operation to be performed.
# Each type has specific configurations in the corresponding section.
task:
  type: "execution"                    # string: Task type
                                      # "execution" = run algorithms with fixed parameters
                                      # "optimization" = optimize hyperparameters with Optuna
                                      # "sensitivity" = analyze sensitivity with SALib

# =====================================================================
# SECTION 6A: SPECIFIC CONFIGURATION - EXECUTION
# =====================================================================
# Use this section when task.type = "execution"
# Runs algorithms on datasets with pre-defined parameters
execution:
  # List of executions to perform
  executions:
    - nome: "Test Execution"           # string: Descriptive execution name
      datasets: ["dataset_teste"]     # list: Dataset IDs to use (reference to section 3)
      algorithms: ["config_padrao"]     # list: Algorithm configuration IDs (reference to section 4)
      repetitions: 1                   # int: Número de repetições por combinação (1-10 recomendado)
                                       # Cada combinação dataset+algoritmo será executada N vezes
    # Você pode adicionar múltiplas execuções:
    # - nome: "Execução Completa"
    #   datasets: ["dataset_teste", "dataset_arquivo"]
    #   algorithms: ["config_padrao"]
    #   repetitions: 5

# =====================================================================
# SECTION 6B: SPECIFIC CONFIGURATION - OPTIMIZATION
# =====================================================================
# Use esta seção quando task.type = "optimization"
# Optimize algorithm hyperparameters using Optuna
optimization:
  # Optimization framework (global configuration)
  method: "optuna"                      # string: Optimization framework (only "optuna" supported)
  
  # Configurações globais do Optuna (aplicadas a todas as otimizações)
  global_optuna_config:
    sampler: "TPESampler"              # string: Algoritmo de amostragem padrão
                                      # "TPESampler" = Tree-structured Parzen Estimator (recomendado)
                                      # "RandomSampler" = Amostragem aleatória
                                      # "CmaEsSampler" = CMA-ES para problemas contínuos
    pruner: "MedianPruner"             # string: Algoritmo de poda padrão
                                      # "MedianPruner" = para com base na mediana
                                      # "SuccessiveHalvingPruner" = halving sucessivo
    n_startup_trials: 10               # int: Trials antes de usar sampler inteligente
    n_warmup_steps: 10                 # int: Steps de warmup para o pruner
    interval_steps: 5                  # int: Intervalo para avaliação do pruner
    multivariate: true                 # bool: Usar TPE multivariado (recomendado)
    n_ei_candidates: 24                # int: Candidatos para Expected Improvement
    storage: null                      # string|null: URL do banco de dados (null = em memória)
  
  # Lista de otimizações a realizar
  optimizations:
    - nome: "BLF-GA Optimization"      # string: Descriptive optimization name
      study_name: "estudo_blfga"       # string: Nome do estudo para identificação
      direction: "minimize"             # string: Optimization direction
                                       # "minimize" = minimizar objetivo (para distância)
                                       # "maximize" = maximizar objetivo (para qualidade)
      n_trials: 100                    # int: Número de trials/tentativas (50-1000 recomendado)
      timeout_per_trial: 300           # int: Timeout por trial em segundos (60-600)
      
      # Datasets e algoritmo específicos para otimizar
      target_datasets: ["dataset_teste"] # list: IDs dos datasets a usar (referência à seção 3)
      target_algorithm: "config_padrao" # string: Algorithm configuration ID (reference to section 4)
      
      # Parâmetros a otimizar (sobrepõem algorithm_params da seção 4)
      # Tipos suportados: int, uniform (float), categorical
      parameters:
        "BLF-GA":                      # string: Algorithm name (must exist in configuration)
          pop_size:                    # Nome do parâmetro (deve existir no algoritmo)
            type: "int"                # string: Tipo do parâmetro
            low: 50                    # int: Valor mínimo (para int/uniform)
            high: 200                  # int: Valor máximo (para int/uniform)
            step: 10                   # int: Passo para valores inteiros (opcional)
          
          max_gens:
            type: "int"
            low: 100
            high: 500
            step: 25
          
          cross_prob:
            type: "uniform"            # float uniforme entre low e high
            low: 0.6
            high: 0.95
          
          mut_prob:
            type: "uniform"
            low: 0.01
            high: 0.3
          
          crossover_type:
            type: "categorical"        # Escolha entre valores discretos
            choices: ["one_point", "uniform", "blend_blocks"]  # list: Opções disponíveis
      
      # Configurações específicas do Optuna (opcional - sobrepõe global_optuna_config)
      optuna_config:
        sampler: "TPESampler"          # string: Algoritmo de amostragem específico
        pruner: "MedianPruner"         # string: Algoritmo de poda específico
        storage: null                  # string|null: URL do banco específico
    
    # Exemplo de segunda otimização (você pode adicionar quantas precisar)
    # - nome: "Otimização CSC"
    #   study_name: "estudo_csc"
    #   direction: "minimize"
    #   n_trials: 50
    #   timeout_per_trial: 180
    #   target_datasets: ["dataset_teste", "dataset_arquivo"]
    #   target_algorithm: "config_padrao"  # Referencia ID de configuração
    #   parameters:
    #     "CSC":                       # Nome do algoritmo na configuração
    #       min_d:
    #         type: "int"
    #         low: 1
    #         high: 5
    #       d_factor:
    #         type: "uniform"
    #         low: 0.5
    #         high: 1.0

# =====================================================================
# SECTION 6C: SPECIFIC CONFIGURATION - SENSITIVITY
# =====================================================================
# Use esta seção quando task.type = "sensitivity"
# Realiza análises de sensibilidade dos parâmetros usando SALib
sensitivity:
  # Framework de análise (configuração global)
  method: "SALib"                       # string: Framework de análise (apenas "SALib" suportado)
  
  # Configurações globais do SALib (aplicadas a todas as análises)
  global_salib_config:
    n_samples: 1000                    # int: Número de amostras padrão para gerar (100-10000)
                                      # Mais amostras = maior precisão mas mais tempo
    repetitions_per_sample: 3          # int: Repetições por amostra padrão (1-10)
                                      # Para reduzir ruído nos resultados
    seed: 42                          # int: Seed global para reprodutibilidade
    parallel: true                    # bool: Usar processamento paralelo
  
  # Lista de análises de sensibilidade a realizar
  analyses:
    - nome: "Análise Morris BLF-GA"    # string: Nome descritivo da análise
      analysis_method: "morris"         # string: Método de análise de sensibilidade
                                      # "morris" = Análise de Morris (elementary effects) - recomendado para screening
                                      # "sobol" = Análise de Sobol (índices de primeira ordem e total)
                                      # "fast" = Análise FAST (Fourier Amplitude Sensitivity)
                                      # "delta" = Delta method
      
      # Datasets e algoritmo específicos para analisar
      target_datasets: ["dataset_teste"] # list: IDs dos datasets a usar (referência à seção 3)
      target_algorithm: "BLF-GA"       # string: Nome do algoritmo a analisar
      
      # Configurações específicas da análise (opcional - sobrepõe global_salib_config)
      n_samples: 1000                  # int: Número de amostras específico
      repetitions_per_sample: 3        # int: Repetições por amostra específico
      
      # Parâmetros a analisar (define os ranges de variação)
      parameters:
        pop_size:                      # Nome do parâmetro
          type: "integer"              # string: Tipo do parâmetro
          bounds: [50, 200]            # list: [min, max] para int/float
          default: 100                 # valor padrão para referência
          
        max_gens:
          type: "integer"
          bounds: [100, 500]
          default: 200
          
        cross_prob:
          type: "float"                # float contínuo
          bounds: [0.6, 0.95]
          default: 0.8
          
        mut_prob:
          type: "float"
          bounds: [0.01, 0.3]
          default: 0.1
          
        crossover_type:
          type: "categorical"          # Valores discretos
          values: ["one_point", "uniform", "blend_blocks"]  # list: Opções a variar
          default: "one_point"
      
      # Configuração específica do método Morris (se analysis_method = "morris")
      morris:
        num_levels: 4                  # int: Níveis para grade Morris (4-10)
        grid_jump: 2                   # int: Tamanho do salto na grade
        num_trajectories: 20           # int: Número de trajetórias a gerar
        optimal_trajectories: null     # int|null: Otimizar trajetórias (null = não otimizar)
      
      # Configuração específica do método Sobol (se analysis_method = "sobol")
      sobol:
        calc_second_order: true        # bool: Calcular índices de segunda ordem
        num_resamples: 1000           # int: Resamples para bootstrap de confiança
        conf_level: 0.95              # float: Nível de confiança (0.90-0.99)
        seed: 42                      # int: Seed para reprodutibilidade
      
      # Configuração específica do método FAST (se analysis_method = "fast")
      fast:
        M: 4                          # int: Fator de frequência interferência
        interference: false           # bool: Considerar interferência de frequência
        
      # Métricas de saída a analisar
      output_metrics:                 # list: Métricas a calcular sensibilidade
        - "distance"                  # Distância máxima alcançada
        - "execution_time"            # Tempo de execução
        # Outras opções: "convergence_rate", "fitness_calls", "diversity"
    
    # Exemplo de segunda análise (você pode adicionar quantas precisar)
    # - nome: "Análise Sobol CSC"
    #   analysis_method: "sobol"
    #   target_datasets: ["dataset_teste", "dataset_arquivo"]
    #   target_algorithm: "CSC"
    #   n_samples: 2000
    #   repetitions_per_sample: 5
    #   parameters:
    #     min_d:
    #       type: "integer"
    #       bounds: [1, 5]
    #       default: 2
    #     d_factor:
    #       type: "float"
    #       bounds: [0.5, 1.0]
    #       default: 0.75
    #   sobol:
    #     calc_second_order: true
    #     num_resamples: 1000
    #     conf_level: 0.95
    #   output_metrics:
    #     - "distance"
    #     - "execution_time"
    #     - "convergence_rate"

# =====================================================================
# SECTION 7: EXPORT SETTINGS (REQUIRED)
# =====================================================================
# Define onde e como os resultados serão salvos
export:
  enabled: true                        # bool: Habilitar exportação de resultados
                                      # true = salvar resultados, false = apenas executar
  
  destination: "outputs/{session_folder}"  # string: Diretório base para salvar
                                      # {session_folder} = pasta da sessão atual (timestamp)
                                      # {task_type} = tipo da tarefa (execution/optimization/sensitivity)
                                      # {algorithm_name} = nome do algoritmo
                                      # {dataset_name} = nome do dataset
  
  # Formatos de saída a gerar
  formats:
    csv: true                          # bool: Exportar resultados em CSV
    json: true                         # bool: Exportar resultados em JSON
    parquet: false                     # bool: Exportar em formato Parquet (para grandes volumes)
    pickle: false                      # bool: Exportar em formato Pickle (Python nativo)
    
  # Configurações específicas por formato
  csv_config:
    separator: ","                     # string: Separador de campos (,;|)
    encoding: "utf-8"                  # string: Codificação de caracteres
    include_index: true                # bool: Incluir índice do DataFrame
    decimal: "."                       # string: Separador decimal
    
  json_config:
    indent: 2                          # int|null: Indentação (null = compacto)
    ensure_ascii: false                # bool: Garantir ASCII (false = permite UTF-8)
    
  # Estrutura de diretórios detalhada
  directory_structure:
    use_algorithm_subfolder: true      # bool: Criar subpasta por algoritmo
    use_dataset_subfolder: true        # bool: Criar subpasta por dataset
    use_timestamp_suffix: true         # bool: Adicionar timestamp aos nomes
    
  # Conteúdo específico a incluir na exportação
  include:                            # list: Tipos de conteúdo a incluir
    - "summary"                       # Resumo executivo dos resultados
    - "detailed_results"              # Resultados detalhados de cada execução
    - "plots"                         # Gráficos e visualizações
    - "logs"                          # Logs de execução
    # Outras opções: "raw_data", "parameters", "metadata"

# =====================================================================
# SECTION 8: VISUALIZATION SETTINGS (OPTIONAL)
# =====================================================================
# Define quais gráficos gerar e suas configurações visuais
plots:
  enabled: true                       # bool: Habilitar geração de gráficos
  
  # Gráficos comuns a todos os tipos de tarefa
  plot_convergence: true              # bool: Gráfico de convergência
  plot_comparison: true               # bool: Comparação entre algoritmos/parâmetros
  plot_boxplots: true                 # bool: Box plots de distribuições
  plot_scatter: true                  # bool: Scatter plots de correlações
  plot_heatmap: true                  # bool: Heatmaps de parâmetros
  plot_runtime: true                  # bool: Análise de tempo de execução
  plot_success_rate: true             # bool: Taxa de sucesso/convergência
  
  # Gráficos específicos por tipo de tarefa
  # Para optimization:
  plot_optimization_history: true    # bool: Histórico de otimização Optuna
  plot_parameter_importance: true    # bool: Importância dos parâmetros
  plot_parallel_coordinate: true     # bool: Gráfico de coordenadas paralelas
  
  # Para sensitivity:
  plot_sensitivity_indices: true     # bool: Índices de sensibilidade
  plot_morris_trajectories: true     # bool: Trajetórias Morris
  plot_interaction_effects: true     # bool: Efeitos de interação
  
  # Configurações visuais gerais
  style: "seaborn-v0_8"              # string: Estilo do matplotlib
                                     # "seaborn-v0_8", "ggplot", "bmh", "classic"
  figure_size: [12, 8]               # list: Tamanho da figura [largura, altura] em polegadas
  dpi: 300                           # int: Resolução em DPI (150-300 recomendado)
  color_palette: "Set2"              # string: Paleta de cores
                                     # "Set1", "Set2", "tab10", "viridis", "plasma"
  
  # Configurações de formato de saída
  formats:                           # list: Formatos para salvar gráficos
    - "png"                          # Formato PNG (recomendado para web)
    - "pdf"                          # Formato PDF (recomendado para publicação)
    # - "svg"                        # Formato SVG (vetorial)
    # - "eps"                        # Formato EPS (para LaTeX)
  
  # Configurações avançadas
  font_size: 12                      # int: Tamanho da fonte padrão
  title_size: 16                     # int: Tamanho do título
  label_size: 14                     # int: Tamanho dos labels dos eixos
  legend_size: 10                    # int: Tamanho da legenda
  tight_layout: true                 # bool: Usar layout apertado automaticamente

# =====================================================================
# SECTION 9: MONITORING SETTINGS (OPTIONAL)
# =====================================================================
# Configurações para monitoramento em tempo real da execução
monitoring:
  # Monitoramento geral
  enabled: true                      # bool: Habilitar monitoramento
  interface: "simple"                # string: Tipo de interface
                                    # "simple" = interface simples no terminal (padrão)
                                    # "tui" = interface avançada com curses
  update_interval: 3                 # int: Intervalo de atualização em segundos (1-10)
  
 
# =====================================================================
# SECTION 10: RESOURCE SETTINGS (OPTIONAL)
# =====================================================================
# Controle de uso de recursos computacionais
resources:
  # Limitações de CPU
  cpu:
    max_cores: null                  # int|null: Máximo de cores a usar (null = todos)
    affinity: null                   # list|null: Cores específicos para usar [0,1,2,3]
    
  # Limitações de memória
  memory:
    max_memory_gb: null              # float|null: Máximo de memória em GB
    
  # Processamento paralelo
  parallel:
    enabled: true                    # bool: Habilitar paralelização quando possível
    max_workers: null                # int|null: Máximo de workers paralelos (null = auto)
    internal_jobs: 4                 # int: Número máximo de jobs paralelos internos por algoritmo
                                    # Controla paralelismo interno do algoritmo (threads/processos)
                                    # Valor sugerido: 4 para CPUs com 8+ cores
                                    # Relação: max_workers × internal_jobs ≤ CPU_cores
    backend: "threading"             # string: Backend de paralelização
                                    # "threading" = threads (I/O bound)
                                    # "multiprocessing" = processos (CPU bound)
    chunk_size: null                # int|null: Tamanho dos chunks para divisão
    
  # Timeouts e limites
  timeouts:
    per_algorithm_run: 3600          # int: Timeout por execução de algoritmo (segundos)
    total_batch: 86400               # int: Timeout total do batch (segundos)
    
  # Configurações de GPU (se disponível)
  gpu:
    enabled: false                   # bool: Usar GPU se disponível
    device_id: 0                     # int: ID do dispositivo GPU a usar

# =====================================================================
# SECTION 11: LOGGING SETTINGS (OPTIONAL)
# =====================================================================
# Controle detalhado dos logs de execução
logging:
  # Nível geral de logging
  level: "INFO"                      # string: Nível de log
                                    # "DEBUG" = logs muito detalhados
                                    # "INFO" = logs informativos (recomendado)
                                    # "WARNING" = apenas avisos e erros
                                    # "ERROR" = apenas erros
  
  # Configurações de saída dos logs
  output:
    console: true                    # bool: Exibir logs no console
    file: true                       # bool: Salvar logs em arquivo
    
  # Configurações do arquivo de log
  file_config:
    filename: "execution.log"        # string: Nome do arquivo de log
    max_size_mb: 100                # int: Tamanho máximo do arquivo em MB
    backup_count: 5                  # int: Número de backups rotativos
    encoding: "utf-8"               # string: Codificação do arquivo
    
  # Formatação das mensagens
  format:
    timestamp_format: "%Y-%m-%d %H:%M:%S"  # string: Formato do timestamp
    message_format: "[{timestamp}] {level} - {message}"  # string: Formato da mensagem
    
  # Logs específicos por componente
  components:
    algorithms: "INFO"               # string: Nível para logs de algoritmos
    infrastructure: "WARNING"       # string: Nível para logs de infraestrutura
    optimization: "INFO"             # string: Nível para logs de otimização
    sensitivity: "INFO"              # string: Nível para logs de análise de sensibilidade
    
  # Configurações avançadas
  filters:
    exclude_patterns: []             # list: Padrões de mensagens a excluir
    include_only: []                 # list: Apenas incluir mensagens com estes padrões
    
  # Logs estruturados (para análise posterior)
  structured:
    enabled: false                   # bool: Habilitar logs estruturados em JSON
    include_context: true            # bool: Incluir contexto extra (parâmetros, etc.)

# =====================================================================
# SECTION 12: SYSTEM SETTINGS (OPTIONAL)
# =====================================================================
# Configurações gerais do sistema e comportamento
system:
  # Controle de reprodutibilidade
  reproducibility:
    global_seed: 42                  # int: Seed global para reprodutibilidade
    strict_mode: true                # bool: Modo estrito (garante mesmo resultado)
    
  # Configurações de checkpoint e recuperação
  checkpointing:
    enabled: true                    # bool: Habilitar salvamento de progresso
    interval: 5                      # int: Intervalo para checkpoint (minutos)
    recovery: true                   # bool: Tentar recuperar de checkpoint anterior
    max_checkpoints: 3               # int: Máximo de checkpoints a manter
    
  # Controle de progresso
  progress:
    show_progress_bar: true          # bool: Exibir barra de progresso
    log_interval: 5                  # int: Intervalo para log de progresso (segundos)
    update_interval: 30              # int: Intervalo para atualização de status (segundos)
    
  # Parada antecipada (para otimização)
  early_stopping:
    enabled: false                   # bool: Habilitar parada antecipada
    patience: 20                     # int: Número de trials sem melhoria
    min_improvement: 0.001           # float: Melhoria mínima considerada significativa
    
  # Configurações de erro e tratamento
  error_handling:
    continue_on_error: false         # bool: Continuar execução mesmo com erros
    max_retries: 3                   # int: Máximo de tentativas em caso de erro
    retry_delay: 5                   # int: Delay entre tentativas (segundos)
    
  # Limpeza automática
  cleanup:
    temp_files: true                 # bool: Limpar arquivos temporários
    old_logs: true                   # bool: Limpar logs antigos
    old_results: false               # bool: Limpar resultados antigos
    retention_days: 30               # int: Dias para manter arquivos antigos

# =====================================================================
# FIM DO TEMPLATE PADRONIZADO
# =====================================================================
# Para usar este template:
# 1. Copie este arquivo e renomeie para sua configuração específica
# 2. Modifique apenas as seções relevantes para seu caso de uso
# 3. Remova as seções não utilizadas (todas as seções opcionais)
# 4. Execute: python main.py seu_arquivo.yaml
# 
# Exemplos prontos disponíveis:
# - processamento_padrao.yaml (execução simples)
# - otimizacao_padrao.yaml (otimização com Optuna)
# - sensibilidade_padrao.yaml (análise de sensibilidade)
#
# =====================================================================
# VARIÁVEIS DE AMBIENTE SUPORTADAS:
# =====================================================================
# - BATCH_TIMEOUT: Timeout global (sobrepõe batch_info.timeout_global)
# - LOG_LEVEL: Nível de logging (sobrepõe logging.level)
# - OUTPUT_PATH: Caminho de saída (sobrepõe export.destination)
# - N_TRIALS: Número de trials para otimização
# - PARALLEL_JOBS: Número de jobs paralelos
# =====================================================================
# DOCUMENTAÇÃO DETALHADA - INTERNAL_JOBS
# =====================================================================
# O parâmetro internal_jobs controla o paralelismo interno dos algoritmos:
#
# 1. Localização: resources.parallel.internal_jobs
#    - Configuração global aplicada a todos os algoritmos
#    - Passado automaticamente para o algoritmo via executor_config
#
# 2. Como o Algoritmo Recebe:
#    executor_config = {
#        "internal_jobs": 4,  # Valor da configuração
#        "resources": {...}   # Configurações completas de recursos
#    }
#
# 3. Uso no Algoritmo (exemplo):
#    ```python
#    class MyAlgorithm:
#        def run(self, executor_config=None):
#            internal_jobs = executor_config.get("internal_jobs", 4)
#            
#            # Para algoritmos genéticos
#            if internal_jobs > 1:
#                # Usar paralelização na avaliação de fitness
#                with ThreadPoolExecutor(max_workers=internal_jobs) as executor:
#                    fitness_values = list(executor.map(evaluate_fitness, population))
#            
#            # Para análise de sensibilidade
#            if internal_jobs > 1:
#                # Paralelizar execuções de amostra
#                with ProcessPoolExecutor(max_workers=internal_jobs) as executor:
#                    results = list(executor.map(run_sample, samples))
#    ```
#
# 4. Considerações de Performance:
#    - max_workers: Controla quantas otimizações/análises executam simultaneamente
#    - internal_jobs: Controla paralelismo interno de cada algoritmo
#    - Fórmula: max_workers × internal_jobs ≤ CPU_cores (evitar overhead)
#
# 5. Valores Recomendados:
#    - CPU 4 cores: max_workers=1, internal_jobs=4
#    - CPU 8 cores: max_workers=2, internal_jobs=4
#    - CPU 16 cores: max_workers=4, internal_jobs=4
#    - CPU 32 cores: max_workers=8, internal_jobs=4
#
# 6. Algoritmos que se Beneficiam:
#    - BLF-GA: Paralelização da avaliação de fitness da população
#    - CSC: Paralelização da busca em blocos
#    - H³-CSP: Paralelização de candidatos
#    - Análises Morris/Sobol: Paralelização de amostras
# =====================================================================
